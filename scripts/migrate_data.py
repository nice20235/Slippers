#!/usr/bin/env python3
"""
Migrate data from a local SQLite DB to a PostgreSQL database.

This tool expects a best-effort Postgres schema at `migrations/pg_schema.sql` (generated by analyze_sqlite.py)
and will create tables (IF NOT EXISTS) and copy data table-by-table.

Usage:
  python3 scripts/migrate_data.py --sqlite ./slippers.db --pg "dbname=mydb user=me password=pass host=localhost port=5432"

Notes:
 - Requires psycopg2 (pip install psycopg2-binary)
 - Runs in transactions per-table (committed). Logs to scripts/logs/migrate.log
 - For complex schema (composite PKs, FKs, triggers) please review `migrations/pg_schema.sql` before running.
"""
import argparse
import sqlite3
import psycopg2
import psycopg2.extras
from datetime import datetime
import os
import logging
import json
from pathlib import Path

LOG_DIR = os.path.join("scripts", "logs")
os.makedirs(LOG_DIR, exist_ok=True)
logger = logging.getLogger("migrate_data")
handler = logging.FileHandler(os.path.join(LOG_DIR, "migrate.log"))
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
logger.addHandler(handler)
logger.setLevel(logging.INFO)


def run_pg_schema(pg_conn, schema_path: str):
    logger.info("Applying PG schema: %s", schema_path)
    with open(schema_path, "r", encoding="utf-8") as f:
        sql = f.read()
    with pg_conn.cursor() as cur:
        cur.execute(sql)
    pg_conn.commit()


def get_pg_column_types(pg_conn, table: str):
    types = {}
    with pg_conn.cursor() as cur:
        cur.execute(
            """
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = %s
            """,
            (table,)
        )
        for name, dtype in cur.fetchall():
            types[name] = dtype
    return types


def normalize_value(val, pg_type: str):
    if val is None:
        return None
    t = (pg_type or '').lower()
    if t == 'boolean':
        if isinstance(val, bool):
            return val
        if isinstance(val, (int,)):
            return bool(val)
        s = str(val).strip().lower()
        if s in ('1', 't', 'true', 'y', 'yes'):
            return True
        if s in ('0', 'f', 'false', 'n', 'no'):
            return False
        # fallback: psycopg2 can sometimes coerce; default to False only if ambiguous
        return False
    # let psycopg handle timestamps/numerics/text
    return val


def migrate_table(sqlite_conn, pg_conn, table: str, batch_size: int = 1000):
    logger.info("Migrating table %s", table)
    scur = sqlite_conn.cursor()
    # get columns
    scur.execute(f"PRAGMA table_info('{table}')")
    cols = scur.fetchall()
    col_names = [c[1] for c in cols]
    col_placeholders = ",".join(["%s"] * len(col_names))
    col_list = ",".join([f'"{c}"' for c in col_names])

    # fetch and insert in batches
    pg_types = get_pg_column_types(pg_conn, table)
    soff = sqlite_conn.cursor()
    soff.execute(f"SELECT COUNT(*) FROM '{table}'")
    total = soff.fetchone()[0]
    logger.info("%s rows to copy for %s", total, table)

    cur = sqlite_conn.cursor()
    cur.execute(f"SELECT {', '.join([f'"{c}"' for c in col_names])} FROM '{table}'")

    inserted = 0
    with pg_conn.cursor() as pgcur:
        while True:
            rows = cur.fetchmany(batch_size)
            if not rows:
                break
            # convert sqlite3.Row to tuple
            batch = []
            for r in rows:
                row_list = []
                for i, v in enumerate(r):
                    col = col_names[i]
                    pg_t = pg_types.get(col)
                    row_list.append(normalize_value(v, pg_t))
                batch.append(tuple(row_list))
            try:
                psycopg2.extras.execute_values(pgcur,
                                               f"INSERT INTO \"{table}\" ({col_list}) VALUES %s ON CONFLICT DO NOTHING",
                                               batch,
                                               template=None,
                                               page_size=1000)
                pg_conn.commit()
                inserted += len(batch)
                logger.info("Inserted %d/%d into %s", inserted, total, table)
            except Exception as e:
                pg_conn.rollback()
                logger.exception("Failed inserting batch into %s: %s", table, e)
                raise

    # attempt to set sequence for integer PKs (if any)
    try:
        pks = [c for c in cols if c[5]]
        if len(pks) == 1:
            pk = pks[0][1]
            with pg_conn.cursor() as curseq:
                curseq.execute(f"SELECT MAX(\"{pk}\") FROM \"{table}\";")
                maxid = curseq.fetchone()[0]
                if maxid is not None:
                    # If the table has an associated sequence (identity/serial), advance it.
                    try:
                        curseq.execute("SELECT pg_get_serial_sequence(%s, %s)", (table, pk))
                        res = curseq.fetchone()
                        if res and res[0]:
                            seq = res[0]
                            curseq.execute("SELECT setval(%s, %s, true)", (seq, int(maxid)))
                            pg_conn.commit()
                    except Exception:
                        # ignore sequence issues on plain BIGINT PKs
                        pg_conn.rollback()
                        logger.exception("Failed to advance sequence for table %s", table)
    except Exception:
        # Defensive: don't break migration flow if any error happens here
        pg_conn.rollback()
        logger.exception("Sequence handling failed for table %s (ignored)", table)


def main():
    parser = argparse.ArgumentParser(description="Migrate SQLite -> PostgreSQL")
    parser.add_argument("--sqlite", default="./slippers.db")
    parser.add_argument("--pg", required=True, help="libpq-style connection string, e.g. 'dbname=mydb user=me password=pass host=127.0.0.1 port=5432'")
    parser.add_argument("--batch-size", type=int, default=1000)
    parser.add_argument("--schema", default="migrations/pg_schema.sql")
    args = parser.parse_args()

    if not Path(args.sqlite).exists():
        raise SystemExit(f"SQLite DB not found: {args.sqlite}")
    if not Path(args.schema).exists():
        raise SystemExit(f"PG schema not found: {args.schema} - run analyze_sqlite.py first")

    sqlite_conn = sqlite3.connect(args.sqlite)
    sqlite_conn.row_factory = sqlite3.Row
    pg_conn = psycopg2.connect(args.pg)

    try:
        run_pg_schema(pg_conn, args.schema)

        # list tables in sqlite
        scur = sqlite_conn.cursor()
        scur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name;")
        tables = [r[0] for r in scur.fetchall()]

        for t in tables:
            migrate_table(sqlite_conn, pg_conn, t, batch_size=args.batch_size)

        logger.info("Migration finished")
        print("Migration finished successfully. See scripts/logs/migrate.log for details.")
    finally:
        sqlite_conn.close()
        pg_conn.close()


if __name__ == "__main__":
    main()
